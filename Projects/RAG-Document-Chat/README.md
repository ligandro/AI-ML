# ğŸ“š RAG Document Chat 

A Streamlit-based AI-powered document assistant for PDF querying using LangChain, Ollama's Llama3.2, and ChromaDB.

<p align="center">
  <img width="70%" src="demo.png"> &nbsp &nbsp
</p>

---

## âš¡ Quick Demo

Upload any PDF â†’ Ask questions â†’ Get accurate, grounded answers

**What it does:**
- Automatically processes and chunks your PDF documents
- Embeds content into a searchable vector database
- Retrieves diverse, relevant information using MMR algorithm or Multi-Query method
- Generates answers grounded solely in document context
- Prevents hallucinations with anti-hallucination prompting

---

## ğŸ¯ Key Features

âœ… **No Hallucinations** - Answers only from document content  
âœ… **Intelligent Retrieval** - MMR algorithm for diverse, relevant results or Multi-Query method
âœ… **Fast Processing** - Efficient PDF chunking and embedding  
âœ… **Session Management** - Auto-cleanup between different PDFs  
âœ… **Interactive UI** - Streamlit interface with sidebar controls  
âœ… **Local LLM** - Runs entirely on Ollama (privacy-first)  
âœ… **Configurable** - Easy-to-modify settings in config.py  

---

## ğŸ—ï¸ How It Works

### Three Simple Stages

**Stage 1: Document Processing**
```
PDF Upload
    â†“
Extract Text
    â†“
Smart Chunking (1200 chars, 300 overlap)
    â†“
Vector Embeddings
```

**Stage 2: Intelligent Retrieval**
```
User Question
    â†“
Find Similar Context (MMR or Multi-Query)
    â†“
Return Top Results
```

**Stage 3: Grounded Answer**
```
Context + Question
    â†“
LLaMA 3.2 (temperature=0)
    â†“
Grounded Answer (no hallucinations)
```

### Why MMR and Multi Query Instead of Similarity?

**Basic Approach:** Pure semantic similarity  
â†’ Returns redundant chunks from same document section

**MMR Approach:** Maximal Marginal Relevance  
â†’ Balances relevance + diversity for comprehensive context (fetches 60, selects top 12)

**Multi-Query Method:**  Generates multiple queries from the question to retrieve a wider range of relevant chunks, further enhancing answer quality.
---

## ğŸ“š Project Structure

```
RAG-Document-Chat/
â”œâ”€â”€ ingest/                    # Document processing pipeline
â”‚   â”œâ”€â”€ load_pdf.py           # PDF loading & cleanup
â”‚   â”œâ”€â”€ chunk_documents.py    # Smart chunking
â”‚   â””â”€â”€ embed_chunks.py       # Embedding & ChromaDB
â”œâ”€â”€ rag/                       # RAG components
â”‚   â”œâ”€â”€ retriever.py          # MMR & Multi-Query retrievers
â”‚   â””â”€â”€ chain.py              # LLM chain with anti-hallucination
â”œâ”€â”€ config.py                  # Centralized configuration
â”œâ”€â”€ app.py                     # Streamlit UI
â””â”€â”€ README.md                  # This file
```

